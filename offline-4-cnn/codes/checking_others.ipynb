{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "input_channels = 3\n",
    "input_height = 24\n",
    "input_width = 24\n",
    "output_channels = 16\n",
    "filter_height = 5\n",
    "filter_width = 5\n",
    "padding = 1\n",
    "stride = 3\n",
    "X = np.random.randint(1,100,(batch_size,input_channels,input_height,input_width))\n",
    "# weight init\n",
    "global_weights = np.random.randn(output_channels, input_channels, filter_height, filter_height) * np.sqrt(2 / (filter_height * filter_height * input_channels))\n",
    "# global_weights_2 = np.random.randn(in_dim, self.out_dim) * np.sqrt(2 / in_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# anan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "def dilateData(input, dilate):\n",
    "    X = input.copy()\n",
    "\n",
    "    if dilate != 0:\n",
    "        for i in range(1, dilate+1):\n",
    "            X = np.insert(X,range(1, X.shape[2],i), 0, axis=2)\n",
    "            X = np.insert(X,range(1, X.shape[3],i), 0, axis=3)\n",
    "    return X\n",
    "\n",
    "def padData(input, padding):\n",
    "    X = input.copy()\n",
    "\n",
    "    return np.pad(X, pad_width=((0,), (0,), (padding,), (padding,)), mode='constant', constant_values=(0.,))\n",
    "\n",
    "def getSlidingWindow(input, output_size, filter_dim, padding=0, stride=1, dilate=0):\n",
    "    X = input\n",
    "\n",
    "    if padding != 0:\n",
    "        X = np.pad(X, pad_width=((0,), (0,), (padding,), (padding,)), mode='constant', constant_values=(0.,))\n",
    "\n",
    "\n",
    "    if dilate != 0:\n",
    "        for i in range(1, dilate+1):\n",
    "            X = np.insert(X,range(1, X.shape[2],i), 0, axis=2)\n",
    "            X = np.insert(X,range(1, X.shape[3],i), 0, axis=3)\n",
    "\n",
    "\n",
    "    in_b, in_c, out_h, out_w = output_size\n",
    "    out_b, out_c, _, _ = input.shape\n",
    "    batch_str, channel_str, filter_height_str, filter_width_str = X.strides\n",
    "\n",
    "    window = np.lib.stride_tricks.as_strided(X, shape=(out_b, out_c, out_h, out_w, filter_dim, filter_dim), strides=(batch_str, channel_str, filter_height_str * stride, filter_width_str * stride, filter_height_str, filter_width_str))\n",
    "    return window\n",
    "\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, out_channels, filter_dim, stride=1, padding=0):\n",
    "        self.out_channels = out_channels\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.input_data = None\n",
    "        self.window = None\n",
    "\n",
    "    def initialize_weights(self, in_channels):\n",
    "        if self.weights is None:\n",
    "            # self.weights = np.random.randn(self.out_channels, in_channels, self.filter_dim, self.filter_dim) * np.sqrt(2 / (self.filter_dim * self.filter_dim * in_channels))\n",
    "            self.weights = global_weights\n",
    "        if self.biases is None:\n",
    "            self.biases = np.zeros(self.out_channels)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        batch_size, in_channels, height, width = input_data.shape\n",
    "        self.initialize_weights(in_channels)\n",
    "\n",
    "        out_height = (height + 2 * self.padding - self.filter_dim) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.filter_dim) // self.stride + 1\n",
    "\n",
    "        window = getSlidingWindow(input_data, (batch_size, in_channels, out_height, out_width), self.filter_dim, self.padding, self.stride)\n",
    "        self.window = window\n",
    "\n",
    "        output = np.einsum('bihwkl,oikl->bohw', window, self.weights) + self.biases[None, :, None, None]\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_grad, learning_rate=0.01):\n",
    "        batch_size, in_channels, height, width = self.input_data.shape\n",
    "\n",
    "        db = np.sum(output_grad, axis=(0, 2, 3))\n",
    "\n",
    "        dilated_dz = dilateData(output_grad, self.stride-1)\n",
    "        padded_X = padData(self.input_data, self.padding)\n",
    "\n",
    "        X_window = getSlidingWindow(padded_X, self.weights.shape, dilated_dz.shape[2],padding=0,stride=1)\n",
    "        dW = np.einsum('bihwkl,bokl->oihw', X_window, dilated_dz)\n",
    "\n",
    "        padded_dilated_dz = padData(dilated_dz, self.filter_dim-1)\n",
    "        rot_kernel = np.rot90(self.weights, 2, (2, 3))\n",
    "\n",
    "        dz_window = getSlidingWindow(padded_dilated_dz, padded_X.shape, self.filter_dim,padding=0,stride=1)\n",
    "\n",
    "        dX = np.einsum('bohwkl,oikl->bihw', dz_window, rot_kernel)\n",
    "\n",
    "        self.weights -= learning_rate * dW / batch_size\n",
    "        self.biases -= learning_rate * db / batch_size\n",
    "\n",
    "        if self.padding > 0:\n",
    "            dX = dX[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
    "\n",
    "        return dX\n",
    "\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.input_data = None\n",
    "        self.output_grad = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        # apply ReLU activation function\n",
    "        return np.maximum(0, input_data)\n",
    "\n",
    "\n",
    "    def backward(self, output_grad, learning_rate=0.01):\n",
    "        delGrad = np.copy(self.input_data)\n",
    "        delGrad[delGrad < 0] = 0\n",
    "        delGrad[delGrad > 0] = 1\n",
    "        delGrad = delGrad * output_grad\n",
    "        return delGrad\n",
    "\n",
    "class MaxPooling:\n",
    "    def __init__(self, filter_dim, stride=1):\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.input_data = None\n",
    "        self.window = None\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        batch_size, in_channels, height, width = input_data.shape\n",
    "\n",
    "        out_height = (height - self.filter_dim) // self.stride + 1\n",
    "        out_width = (width - self.filter_dim) // self.stride + 1\n",
    "\n",
    "        window = getSlidingWindow(input_data, (batch_size, in_channels, out_height, out_width), self.filter_dim, stride=self.stride)\n",
    "        self.window = window\n",
    "\n",
    "        output = np.max(window, axis=(4, 5))\n",
    "\n",
    "        maxs = output.repeat(self.stride, axis=2).repeat(self.stride, axis=3)\n",
    "        x_window = self.input_data[:, :, :out_height * self.stride, :out_width * self.stride]\n",
    "\n",
    "        mask = (x_window == maxs)\n",
    "        self.mask = mask\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_grad, learning_rate=0.01):\n",
    "        X = self.input_data\n",
    "        batch_size, in_channels, height, width = X.shape\n",
    "        h_pool, w_pool = self.filter_dim, self.filter_dim\n",
    "\n",
    "        mask = self.mask\n",
    "        dA = output_grad.repeat(h_pool, axis=2).repeat(w_pool, axis=3)\n",
    "        dA = np.multiply(dA, mask)\n",
    "\n",
    "        pad = np.zeros(X.shape)\n",
    "        pad[:, :, :dA.shape[2], :dA.shape[3]] = dA\n",
    "\n",
    "        return pad\n",
    "\n",
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.input_data = None\n",
    "        self.output_grad = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        return input_data.reshape(input_data.shape[0], -1)\n",
    "\n",
    "    def backward(self, output_grad, learning_rate=0.01):\n",
    "        return output_grad.reshape(self.input_data.shape)\n",
    "\n",
    "class FullyConnected:\n",
    "    def __init__(self, out_dim):\n",
    "        self.out_dim = out_dim\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.input_data = None\n",
    "        self.output_grad = None\n",
    "\n",
    "    def initialize_weights(self, in_dim):\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(in_dim, self.out_dim) * np.sqrt(2 / in_dim)\n",
    "        if self.biases is None:\n",
    "            self.biases = np.zeros(self.out_dim)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        self.initialize_weights(input_data.shape[1])\n",
    "        return np.dot(input_data, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, output_grad, learning_rate=0.01):\n",
    "        self.output_grad = output_grad\n",
    "        self.weights -= learning_rate * np.dot(self.input_data.T, output_grad)\n",
    "        self.biases -= learning_rate * np.sum(output_grad, axis=0)\n",
    "        return np.dot(output_grad, self.weights.T)\n",
    "\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.input_data = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        exp = np.exp(input_data - np.max(input_data, axis=1, keepdims=True))\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, output_grad, learning_rate=0.01):\n",
    "        delU = np.copy(self.input_data)\n",
    "        return delU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# asif conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from layer import Layer\n",
    "\n",
    "class AS_Convolution():\n",
    "    def __init__(self, num_filters, filter_dim, stride=1, padding=0):\n",
    "        # num_filters: number of output channels\n",
    "        # filter_dim: filter dimension (height, width)\n",
    "        # stride: stride of the convolution. default: 1\n",
    "        # padding: padding of the input (assuming zero padding and square padding). default: 0\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_height, self.filter_width = filter_dim\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # initialize weights and biases\n",
    "        # None, as we don't know the input shape yet\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input: (batch_size, num_channels, input_height, input_width)\n",
    "        batch_size, num_channels, input_height, input_width = input.shape\n",
    "\n",
    "        # weights: (num_filters, num_channels, filter_height, filter_width)\n",
    "        # initialize weights xaiver initialization\n",
    "        if self.weights is None:\n",
    "            # self.weights = np.random.randn(self.num_filters, num_channels, self.filter_height, self.filter_width) * (\n",
    "            #     np.sqrt(2 / (\n",
    "            #         self.filter_height\n",
    "            #     ))\n",
    "            # )\n",
    "            self.weights = global_weights\n",
    "\n",
    "        # biases: (num_filters, 1)\n",
    "        # initialize biases to 0\n",
    "        if self.biases is None:\n",
    "            self.biases = np.zeros(self.num_filters)\n",
    "\n",
    "        # output: (batch_size, num_filters, output_height, output_width)\n",
    "        output_height = int((input_height - self.filter_height +\n",
    "                            2 * self.padding) / self.stride + 1)\n",
    "        output_width = int((input_width - self.filter_width + 2 *\n",
    "                           self.padding) / self.stride + 1)\n",
    "\n",
    "        # np.pad: Number of values padded to the edges of each axis.\n",
    "        # ((before_1, after_1), ... (before_N, after_N)) unique pad widths for each axis.\n",
    "        # pad with 'constant' values. default: 0\n",
    "        input_padded = np.pad(input, ((0, 0),\n",
    "                                      (0, 0),\n",
    "                                      (self.padding, self.padding),\n",
    "                                      (self.padding, self.padding)\n",
    "                                      ), 'constant')\n",
    "\n",
    "        # # for loop\n",
    "        #\n",
    "        # output = np.zeros((batch_size, self.num_filters,\n",
    "        #                   output_height, output_width))\n",
    "        #\n",
    "        # for sample_index in range(batch_size):\n",
    "        #     for filter_index in range(self.num_filters):\n",
    "        #         for h in range(output_height):\n",
    "        #             for w in range(output_width):\n",
    "        #                 output[sample_index, filter_index, h, w] = np.sum(\n",
    "        #                     input_padded[sample_index,\n",
    "        #                                  :,\n",
    "        #                                  h * self.stride:h * self.stride + self.filter_height,\n",
    "        #                                  w * self.stride:w * self.stride + self.filter_width\n",
    "        #                                  ] * self.weights[filter_index]\n",
    "        #                 ) + self.biases[filter_index]\n",
    "\n",
    "        # as strided\n",
    "        # https://stackoverflow.com/a/53099870\n",
    "        input_strided = np.lib.stride_tricks.as_strided(\n",
    "            input_padded,\n",
    "            shape=(\n",
    "                batch_size,\n",
    "                num_channels,\n",
    "                output_height,\n",
    "                output_width,\n",
    "                self.filter_height,\n",
    "                self.filter_width\n",
    "            ),\n",
    "            strides=(\n",
    "                input_padded.strides[0],\n",
    "                input_padded.strides[1],\n",
    "                input_padded.strides[2] * self.stride,\n",
    "                input_padded.strides[3] * self.stride,\n",
    "                input_padded.strides[2],\n",
    "                input_padded.strides[3]\n",
    "            )\n",
    "        )\n",
    "        # einsum\n",
    "        # https://ajcr.net/Basic-guide-to-einsum/\n",
    "        output = np.einsum(\n",
    "            'bcijkl,fckl->bfij',\n",
    "            input_strided, self.weights\n",
    "        )\n",
    "        output += self.biases.reshape(1, -1, 1, 1)\n",
    "\n",
    "        # self.cache = (input, input_padded, input_strided)\n",
    "        self.cache = input_padded\n",
    "        # self.cache = input\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        # output_error: (batch_size, num_filters, output_height, output_width)\n",
    "        # learning_rate: learning rate\n",
    "        # input, input_padded, input_strided = self.cache\n",
    "        input_padded = self.cache\n",
    "        # input = self.cache\n",
    "\n",
    "        # dilate the output error\n",
    "        dilate = self.stride - 1\n",
    "        # insert dilate number of 0 rows/cols between each row/col\n",
    "        output_error_modified = np.insert(\n",
    "            output_error,\n",
    "            obj=np.arange(1, output_error.shape[2]).repeat(dilate),\n",
    "            values=0,\n",
    "            axis=2\n",
    "        )\n",
    "        output_error_modified = np.insert(\n",
    "            output_error_modified,\n",
    "            obj=np.arange(1, output_error_modified.shape[3]).repeat(dilate),\n",
    "            values=0,\n",
    "            axis=3\n",
    "        )\n",
    "\n",
    "        weights_error_height = input_padded.shape[2] - \\\n",
    "            output_error_modified.shape[2] + 1\n",
    "        weights_error_width = input_padded.shape[3] - \\\n",
    "            output_error_modified.shape[3] + 1\n",
    "\n",
    "        # check with original weights\n",
    "        # dilate the output error at the end\n",
    "        output_error_modified_weights = output_error_modified.copy()\n",
    "        if weights_error_height > self.weights.shape[2]:\n",
    "            output_error_modified_weights = np.insert(\n",
    "                output_error_modified,\n",
    "                obj=np.array([output_error_modified.shape[2]]).repeat(\n",
    "                    weights_error_height - self.weights.shape[2]\n",
    "                ),\n",
    "                values=0,\n",
    "                axis=2\n",
    "            )\n",
    "        if weights_error_width > self.weights.shape[3]:\n",
    "            output_error_modified_weights = np.insert(\n",
    "                output_error_modified_weights,\n",
    "                obj=np.array([output_error_modified_weights.shape[3]]).repeat(\n",
    "                    weights_error_width - self.weights.shape[3]\n",
    "                ),\n",
    "                values=0,\n",
    "                axis=3\n",
    "            )\n",
    "\n",
    "        # update weights\n",
    "        # as strided\n",
    "        input_strided = np.lib.stride_tricks.as_strided(\n",
    "            input_padded,\n",
    "            shape=(\n",
    "                input_padded.shape[0],\n",
    "                input_padded.shape[1],\n",
    "                self.weights.shape[2],\n",
    "                self.weights.shape[3],\n",
    "                output_error_modified_weights.shape[2],\n",
    "                output_error_modified_weights.shape[3]\n",
    "            ),\n",
    "            strides=(\n",
    "                input_padded.strides[0],\n",
    "                input_padded.strides[1],\n",
    "                input_padded.strides[2],\n",
    "                input_padded.strides[3],\n",
    "                input_padded.strides[2],\n",
    "                input_padded.strides[3]\n",
    "            )\n",
    "        )\n",
    "        weights_error = np.einsum(\n",
    "            'bcklij,bfij->fckl',\n",
    "            input_strided, output_error_modified_weights\n",
    "        )\n",
    "        weights_error = weights_error * 1/input_padded.shape[0]\n",
    "        self.weights -= learning_rate * weights_error\n",
    "\n",
    "        # update biases\n",
    "        biases_error = np.sum(output_error_modified,\n",
    "                              axis=(0, 2, 3)) * 1/input_padded.shape[0]\n",
    "        self.biases -= learning_rate * biases_error\n",
    "\n",
    "        # input error calculation\n",
    "        padded_height = output_error_modified.shape[2] + \\\n",
    "            2 * (self.filter_height - 1)\n",
    "        padded_width = output_error_modified.shape[3] + \\\n",
    "            2 * (self.filter_width - 1)\n",
    "\n",
    "        input_error_height = padded_height - self.filter_height + 1\n",
    "        input_error_width = padded_width - self.filter_width + 1\n",
    "\n",
    "        # check with the original input\n",
    "        # dilate the output error at the end\n",
    "        if input_error_height < input_padded.shape[2]:\n",
    "            output_error_modified = np.insert(\n",
    "                output_error_modified,\n",
    "                obj=np.array([output_error_modified.shape[2]]).repeat(\n",
    "                    input_padded.shape[2] - input_error_height),\n",
    "                values=0,\n",
    "                axis=2\n",
    "            )\n",
    "        if input_error_width < input_padded.shape[3]:\n",
    "            output_error_modified = np.insert(\n",
    "                output_error_modified,\n",
    "                obj=np.array([output_error_modified.shape[3]]).repeat(\n",
    "                    input_padded.shape[3] - input_error_width),\n",
    "                values=0,\n",
    "                axis=3\n",
    "            )\n",
    "\n",
    "        # pad the output error with filter_size - 1\n",
    "        output_error_modified = np.pad(\n",
    "            output_error_modified,\n",
    "            (\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (self.filter_height - 1, self.filter_height - 1),\n",
    "                (self.filter_width - 1, self.filter_width - 1)\n",
    "            ),\n",
    "            'constant'\n",
    "        )\n",
    "        # rotate the weights by 180\n",
    "        weights_modified = np.rot90(self.weights, 2, (2, 3))\n",
    "\n",
    "        # assert input_padded.shape[2] == output_error_modified.shape[2] - \\\n",
    "        #     self.filter_height + 1\n",
    "        # assert input_padded.shape[3] == output_error_modified.shape[3] - \\\n",
    "        #     self.filter_width + 1\n",
    "        # as strided\n",
    "        output_error_modified_strided = np.lib.stride_tricks.as_strided(\n",
    "            output_error_modified,\n",
    "            shape=(\n",
    "                output_error_modified.shape[0],\n",
    "                output_error_modified.shape[1],\n",
    "                input_padded.shape[2],\n",
    "                input_padded.shape[3],\n",
    "                self.filter_height,\n",
    "                self.filter_width\n",
    "            ),\n",
    "            strides=(\n",
    "                output_error_modified.strides[0],\n",
    "                output_error_modified.strides[1],\n",
    "                output_error_modified.strides[2],\n",
    "                output_error_modified.strides[3],\n",
    "                output_error_modified.strides[2],\n",
    "                output_error_modified.strides[3]\n",
    "            )\n",
    "        )\n",
    "        # einsum\n",
    "        input_error = np.einsum(\n",
    "            'bfijkl,fckl->bcij',\n",
    "            output_error_modified_strided, weights_modified\n",
    "        )\n",
    "\n",
    "        # drop the padded rows/cols\n",
    "        input_error = input_error[:, :, self.padding:-\n",
    "                                  self.padding, self.padding:-self.padding]\n",
    "\n",
    "        # # clear cache\n",
    "        # input_padded = None\n",
    "        # self.cache = None\n",
    "\n",
    "        return input_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utsa conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    " \n",
    "# np.random.seed(42)\n",
    " \n",
    " \n",
    "def getSlidingWindow(input, output_size, filter_dim, padding=0, stride=1, dilate=0):\n",
    "    X = input\n",
    " \n",
    "    if padding != 0:\n",
    "        X = np.pad(X, pad_width=((0,), (0,), (padding,), (padding,)),\n",
    "                   mode='constant', constant_values=(0.,))\n",
    " \n",
    "    if dilate != 0:\n",
    "        for i in range(1, dilate+1):\n",
    "            X = np.insert(X, range(1, X.shape[0], i), 0, axis=0)\n",
    "            X = np.insert(X, range(1, X.shape[1], i), 0, axis=1)\n",
    " \n",
    "    in_b, in_c, out_h, out_w = output_size\n",
    "    out_b, out_c, _, _ = input.shape\n",
    "    batch_str, channel_str, filter_height_str, filter_width_str = X.strides\n",
    " \n",
    "    window = np.lib.stride_tricks.as_strided(X, shape=(out_b, out_c, out_h, out_w, filter_dim, filter_dim), strides=(\n",
    "        batch_str, channel_str, filter_height_str * stride, filter_width_str * stride, filter_height_str, filter_width_str))\n",
    "    return window\n",
    " \n",
    " \n",
    "def dilateKoreDao(input, dilatePoriman):\n",
    "    X = input.copy()\n",
    " \n",
    "    if dilatePoriman != 0:\n",
    "        for i in range(1, dilatePoriman+1):\n",
    "            X = np.insert(X, range(1, X.shape[2], i), 0, axis=2)\n",
    "            X = np.insert(X, range(1, X.shape[3], i), 0, axis=3)\n",
    " \n",
    "    return X\n",
    " \n",
    " \n",
    "def padKoreDao(input, padding):\n",
    " \n",
    "    return np.pad(input, pad_width=((0,), (0,), (padding,), (padding,)),\n",
    "                  mode='constant', constant_values=(0.,))\n",
    " \n",
    " \n",
    "class Convolution:\n",
    "    def __init__(self, out_channels, filter_dim, stride=1, padding=0):\n",
    "        self.out_channels = out_channels\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.input_data = None\n",
    "        self.window = None\n",
    " \n",
    "    def initialize_weights(self, in_channels):\n",
    "        if self.weights is None:\n",
    "            # self.weights = np.random.randn(self.out_channels, in_channels, self.filter_dim,\n",
    "            #                                self.filter_dim) * np.sqrt(2 / (self.filter_dim * self.filter_dim * in_channels))\n",
    "            self.weights = global_weights\n",
    "        if self.biases is None:\n",
    "            self.biases = np.zeros(self.out_channels)\n",
    " \n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        batch_size, in_channels, height, width = input_data.shape\n",
    "        self.initialize_weights(in_channels)\n",
    " \n",
    "        out_height = (height + 2 * self.padding -\n",
    "                      self.filter_dim) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding -\n",
    "                     self.filter_dim) // self.stride + 1\n",
    " \n",
    "        window = getSlidingWindow(input_data, (batch_size, in_channels,\n",
    "                                  out_height, out_width), self.filter_dim, self.padding, self.stride)\n",
    "        self.window = window\n",
    " \n",
    "        output = np.einsum('bihwkl,oikl->bohw', window,\n",
    "                           self.weights) + self.biases[None, :, None, None]\n",
    " \n",
    "        return output\n",
    " \n",
    "    def backward(self, output_grad, learning_rate=0.01):\n",
    "        # if self.padding == 0:\n",
    "        #     padding = self.filter_dim - 1\n",
    "        # else:\n",
    "        #     padding = self.padding\n",
    " \n",
    "        batch_size, in_channels, height, width = self.input_data.shape\n",
    " \n",
    "        # d_window = getSlidingWindow(output_grad, self.input_data.shape, self.filter_dim, padding=self.stride-1, stride=1, dilate=self.stride - 1)\n",
    "        # rot_kernel = np.rot90(self.weights, 2, axes=(2, 3))\n",
    " \n",
    "        # db = np.sum(output_grad, axis=(0, 2, 3))\n",
    "        # self.biases -= learning_rate * db / batch_size\n",
    " \n",
    "        # output_grad dilate\n",
    "        # dilate = stride - 1\n",
    "        # dilated_output_\n",
    " \n",
    "        # convolve dilated_output_grad with padded_input\n",
    "        # strided view of dilated_output_grad\n",
    "        # einsum\n",
    " \n",
    "        # dw = np.einsum('bihwkl,bohw->oikl', self.window, output_grad)\n",
    "        # dx = np.einsum('bohwkl,oikl->bihw', d_window, rot_kernel)\n",
    " \n",
    "        # self.weights -= learning_rate * dw\n",
    "        # self.biases -= learning_rate * db\n",
    " \n",
    "        db = np.sum(output_grad, axis=(0, 2, 3))\n",
    " \n",
    "        dilated_dz = dilateKoreDao(output_grad, self.stride-1)\n",
    " \n",
    "        padded_X = padKoreDao(self.input_data, self.padding)\n",
    " \n",
    "        X_window = getSlidingWindow(\n",
    "            padded_X, self.weights.shape, dilated_dz.shape[2], padding=0, stride=1)\n",
    " \n",
    "        dw = np.einsum('bihwkl,bokl->oihw', X_window, dilated_dz)\n",
    " \n",
    "        padded_dilated_dz = padKoreDao(dilated_dz, self.filter_dim-1)\n",
    " \n",
    "        rot_kernel = np.rot90(self.weights, 2, axes=(2, 3))\n",
    " \n",
    "        dz_window = getSlidingWindow(\n",
    "            padded_dilated_dz, padded_X.shape, self.filter_dim, padding=0, stride=1)\n",
    "        dx = np.einsum('bohwkl,oikl->bihw', dz_window, rot_kernel)\n",
    " \n",
    "        if self.padding > 0:\n",
    "            # drop padding from dx\n",
    "            dx = dx[:, :, self.padding:-self.padding,\n",
    "                    self.padding:-self.padding]\n",
    " \n",
    "        # update\n",
    "        self.weights -= learning_rate * dw / batch_size\n",
    "        self.biases -= learning_rate * db / batch_size\n",
    " \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nawmi conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FN_ConvLayer:\n",
    "    def __init__(self, num_filters, filter_dim, stride, padding):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.input = None\n",
    "        self.filters = None\n",
    "        self.bias = np.zeros(num_filters)\n",
    "        # self.left_gradient =   None\n",
    "        # self.filters_gradient = None\n",
    "        # self.bias_gradient = None\n",
    "        self.learning_rate = 0.001\n",
    "        self.out_h = None\n",
    "        self.out_w = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "\n",
    "        if self.filters is None:\n",
    "            # self.filters = np.random.randn(self.num_filters, input.shape[1], self.filter_dim, self.filter_dim) * np.sqrt(2 / (self.num_filters * self.filter_dim * self.filter_dim))\n",
    "            self.filters = global_weights\n",
    "\n",
    "        #output dimensions\n",
    "        self.out_h = int(\n",
    "            (\n",
    "                (input.shape[2] - self.filter_dim + 2 * self.padding) / self.stride\n",
    "            ) + 1\n",
    "        )\n",
    "        self.out_w = int(((input.shape[3] - self.filter_dim + 2 * self.padding) / self.stride) + 1)\n",
    "\n",
    "        #padding\n",
    "        padded_input = np.pad(input, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), 'constant')\n",
    "\n",
    "        #shape for strided input --> (batch_size, num_filters, out_h, out_w, filter_dim, filter_dim)\n",
    "        strided_input = np.lib.stride_tricks.as_strided(padded_input, \n",
    "                                                        shape=(\n",
    "                                                            padded_input.shape[0],\n",
    "                                                            padded_input.shape[1],\n",
    "                                                            self.out_h,\n",
    "                                                            self.out_w,\n",
    "                                                            self.filter_dim,\n",
    "                                                            self.filter_dim\n",
    "                                                        ), \n",
    "                                                        strides=(\n",
    "                                                            padded_input.strides[0],\n",
    "                                                            padded_input.strides[1],\n",
    "                                                            padded_input.strides[2] * self.stride,\n",
    "                                                            padded_input.strides[3] * self.stride,\n",
    "                                                            padded_input.strides[2],\n",
    "                                                            padded_input.strides[3]\n",
    "                                                        )\n",
    "                                                    )\n",
    "        \n",
    "        \n",
    "        output = np.einsum('bchwkl, nckl -> bnhw', strided_input, self.filters) + self.bias.reshape(1, self.num_filters, 1, 1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# running conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "asif_conv = AS_Convolution(num_filters=output_channels, filter_dim=(filter_height,filter_width), stride=stride, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "asif_z = asif_conv.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 16, 8, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asif_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nawmi_conv = FN_ConvLayer(num_filters=output_channels, filter_dim=filter_height, stride=stride, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nawmi_z = nawmi_conv.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 4, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nawmi_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(asif_z, nawmi_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "anan_conv = Convolution(out_channels=output_channels, filter_dim=filter_height, stride=stride, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "anan_z = anan_conv.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 16, 8, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anan_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(asif_z,anan_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "utsa_conv = Convolution(out_channels=output_channels, filter_dim=filter_height, stride=stride, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "utsa_z = utsa_conv.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 16, 8, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utsa_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(asif_z,utsa_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "asif_dx = asif_conv.backward(asif_z, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 24, 24)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asif_dx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "utsa_dx = utsa_conv.backward(utsa_z, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 24, 24)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utsa_dx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(asif_dx, utsa_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "anan_dx = anan_conv.backward(asif_z, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 24, 24)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anan_dx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(asif_dx,anan_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utsa maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSlidingWindow(input, output_size, filter_size, padding=0, stride=1, dilate=False):\n",
    " \n",
    "    # input X - input size of (B,C,H,W) - B=Batch,C=Channels,H=height,W=width\n",
    "    # output_size - (output_height,output_width)\n",
    "    # filter_size - (K,L) - K=kernel height,L=kernel width\n",
    " \n",
    "    modified_input = input\n",
    " \n",
    "    # dilate the input if necessary\n",
    " \n",
    "    if dilate == True:\n",
    "        modified_input = np.insert(\n",
    "            modified_input, range(1, input.shape[2]), 0, axis=2)\n",
    "        modified_input = np.insert(\n",
    "            modified_input, range(1, input.shape[3], 0, axis=3))\n",
    " \n",
    " \n",
    "    # padding of the input\n",
    " \n",
    "    if padding != 0:\n",
    "        modified_input = np.pad(modified_input, pad_width=((0,), (0,), (padding,), (padding,)), mode='constant', constant_values=(0.,))\n",
    " \n",
    "    # set the size for stride  \n",
    "    output_height, output_width = output_size\n",
    "    output_batch, output_channel, _, _ = input.shape\n",
    "    batch_stride, channel_stride, filter_height_stride, filter_width_stride = modified_input.strides\n",
    "    filter_height, filter_width = filter_size,filter_size\n",
    " \n",
    "    # bihwkl - batch, input channel, output height, output width, filter height, filter width\n",
    "    # stride of the input\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        modified_input,\n",
    "        (output_batch, output_channel, output_height, output_width, filter_height, filter_width),\n",
    "        (batch_stride, channel_stride, stride*filter_height_stride, stride*filter_width_stride, filter_height_stride, filter_width_stride)\n",
    "    )\n",
    " \n",
    " \n",
    "class UT_Maxpool:\n",
    "    # implementation of a maxpooling layer. maxpool input with output_channels different filters and stride\n",
    "    # each filter spans all channels in the input\n",
    " \n",
    "    def __init__(self, filter_size, stride):\n",
    "        # filter_size: the specified size of the kernel(both height and width) ex : 2x2 filter with height 2 and width 2\n",
    "        # stride: the stride of convolution\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.cache = None\n",
    " \n",
    "    def forward(self, input):\n",
    "        B, C, H, W = input.shape\n",
    "        #print(\"Maxpool Input Shape : \", H, W)\n",
    "        H_out = (H - self.filter_size)//self.stride + 1\n",
    "        W_out = (W - self.filter_size)//self.stride + 1\n",
    "        #print(\"Maxpool Output Shape : \", H_out, W_out)\n",
    "        sliding_window = getSlidingWindow(\n",
    "            input, (H_out, W_out), self.filter_size, 0, self.stride)\n",
    "        maxpool_output = np.max(sliding_window, axis=(4, 5))\n",
    "        # print(maxpool_output.shape)\n",
    " \n",
    "        # cahche the input and the sliding_window for use in back propagation\n",
    "        self.cache = input, sliding_window\n",
    " \n",
    "        return maxpool_output\n",
    "    \n",
    "    # def backward(self, d_output):\n",
    "    #     input, sliding_window = self.cache\n",
    "    #     B, C, H, W = input.shape\n",
    "    #     H_out = (H - self.filter_size)//self.stride + 1\n",
    "    #     W_out = (W - self.filter_size)//self.stride + 1\n",
    " \n",
    "    #     d_input = np.zeros_like(input)\n",
    "    #     for i in range(B):\n",
    "    #         for j in range(C):\n",
    "    #             for k in range(H_out):\n",
    "    #                 for l in range(W_out):\n",
    "    #                     # get the index of the max value in the sliding window\n",
    "    #                     max_index = np.argmax(\n",
    "    #                         sliding_window[i, j, k, l, :, :])\n",
    "    #                     # get the row and column of the max value in the sliding window\n",
    "    #                     max_row = max_index // self.filter_size\n",
    "    #                     max_col = max_index % self.filter_size\n",
    "    #                     # add the gradient to the corresponding index in the input\n",
    "    #                     d_input[i, j, k*self.stride+max_row, l *\n",
    "    #                             self.stride+max_col] += d_output[i, j, k, l]\n",
    " \n",
    "    #     return d_input\n",
    " \n",
    "    def backward(self, d_output):\n",
    "        B,C,H,W = self.cache[0].shape\n",
    "        H_out = (H - self.filter_size)//self.stride + 1\n",
    "        W_out = (W - self.filter_size)//self.stride + 1\n",
    "        d_input = np.zeros_like(self.cache[0])\n",
    " \n",
    "        for i in range(B):\n",
    "            for j in range(C):\n",
    "                for k in range(H_out):\n",
    "                    for l in range(W_out):\n",
    "                        # create the current window with stride\n",
    "                        current_window = self.cache[0][i, j, k*self.stride:k*self.stride+self.filter_size, l*self.stride:l*self.stride+self.filter_size]\n",
    "                        # get the index of the max value in the current window\n",
    "                        max_val = np.max(current_window)\n",
    "                        # update the d_input where the max value is located\n",
    "                        d_input[i, j, k*self.stride:k*self.stride+self.filter_size, l*self.stride:l*self.stride+self.filter_size] = (current_window == max_val) * d_output[i, j, k, l]\n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# asif maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from layer import Layer\n",
    "\n",
    "class AS_MaxPooling():\n",
    "    def __init__(self, filter_dim, stride):\n",
    "        # filter_dim: filter dimension. (filter_height, filter_width)\n",
    "\n",
    "        self.filter_height, self.filter_width = filter_dim\n",
    "        self.stride = stride\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, num_channels, height, width = input.shape\n",
    "\n",
    "        output_height = int(\n",
    "            (height - self.filter_height) / self.stride + 1)\n",
    "        output_width = int(\n",
    "            (width - self.filter_width) / self.stride + 1)\n",
    "\n",
    "        # # for loop\n",
    "        # output = np.zeros(\n",
    "        #     (batch_size, num_channels, output_height, output_width))\n",
    "        # for h in range(output_height):\n",
    "        #     for w in range(output_width):\n",
    "        #         strided_window = input[\n",
    "        #             :,\n",
    "        #             :,\n",
    "        #             h * self.stride:h * self.stride + self.filter_height,\n",
    "        #             w * self.stride:w * self.stride + self.filter_width\n",
    "        #         ]\n",
    "        #         output[:, :, h, w] = np.max(strided_window, axis=(2, 3))\n",
    "\n",
    "        # as strided\n",
    "        input_strided = np.lib.stride_tricks.as_strided(\n",
    "            input,\n",
    "            shape=(\n",
    "                batch_size,\n",
    "                num_channels,\n",
    "                output_height,\n",
    "                output_width,\n",
    "                self.filter_height,\n",
    "                self.filter_width\n",
    "            ),\n",
    "            strides=(\n",
    "                input.strides[0],\n",
    "                input.strides[1],\n",
    "                input.strides[2] * self.stride,\n",
    "                input.strides[3] * self.stride,\n",
    "                input.strides[2],\n",
    "                input.strides[3]\n",
    "            )\n",
    "        )\n",
    "        output = np.max(input_strided, axis=(4, 5))\n",
    "\n",
    "        # special case when stride == filter_size\n",
    "        max_value_mask = None\n",
    "        # max_value_mask -> (batch_size, num_channels, input_height, input_width)\n",
    "        if self.stride == self.filter_height:\n",
    "            # initialize from output. repeating\n",
    "            max_value_mask = output.repeat(\n",
    "                self.stride,\n",
    "                axis=2\n",
    "            ).repeat(\n",
    "                self.stride,\n",
    "                axis=3\n",
    "            )\n",
    "            # pad for non-divisible input size\n",
    "            max_pad_height = height - max_value_mask.shape[2]\n",
    "            max_pad_width = width - max_value_mask.shape[3]\n",
    "            if max_pad_height > 0 or max_pad_width > 0:\n",
    "                max_value_mask = np.pad(\n",
    "                    max_value_mask,\n",
    "                    (\n",
    "                        (0, 0),\n",
    "                        (0, 0),\n",
    "                        (0, max_pad_height),\n",
    "                        (0, max_pad_width)\n",
    "                    ),\n",
    "                    'constant',\n",
    "                )\n",
    "            # compare with input\n",
    "            # problem for multiple maxima :(\n",
    "            max_value_mask = np.equal(max_value_mask, input)\n",
    "\n",
    "        self.cache = input, max_value_mask\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        # output_error. (batch_size, num_channels, out_height, out_width)\n",
    "        input, max_value_mask = self.cache\n",
    "        batch_size, num_channels, height, width = input.shape\n",
    "        # special case when stride == filter_size\n",
    "        if self.stride == self.filter_height:\n",
    "            # check if max_value_mask is not None\n",
    "            if max_value_mask is None:\n",
    "                raise Exception(\n",
    "                    'max_value_mask is None. Check if stride == filter_height')\n",
    "            else:\n",
    "                # output error needs to be tiled\n",
    "                repeated_output_error = output_error.repeat(\n",
    "                    self.stride,\n",
    "                    axis=2\n",
    "                ).repeat(\n",
    "                    self.stride,\n",
    "                    axis=3\n",
    "                )\n",
    "                # pad for non-divisible input size\n",
    "                pad_height = height - repeated_output_error.shape[2]\n",
    "                pad_width = width - repeated_output_error.shape[3]\n",
    "                if pad_height > 0 or pad_width > 0:\n",
    "                    repeated_output_error = np.pad(\n",
    "                        repeated_output_error,\n",
    "                        (\n",
    "                            (0, 0),\n",
    "                            (0, 0),\n",
    "                            (0, pad_height),\n",
    "                            (0, pad_width)\n",
    "                        ),\n",
    "                        'constant',\n",
    "                    )\n",
    "                # element-wise multiplication\n",
    "                input_error = np.einsum(\n",
    "                    'ijkl,ijkl->ijkl',\n",
    "                    max_value_mask,\n",
    "                    repeated_output_error\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # for loop\n",
    "            _, _, output_height, output_width = output_error.shape\n",
    "            input_error = np.zeros(input.shape)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                for j in range(num_channels):\n",
    "                    for h in range(output_height):\n",
    "                        for w in range(output_width):\n",
    "                            input_window = input[\n",
    "                                i,\n",
    "                                j,\n",
    "                                h*self.stride:h*self.stride+self.filter_height,\n",
    "                                w*self.stride:w*self.stride+self.filter_width\n",
    "                            ]\n",
    "                            # https://stackoverflow.com/a/9483964\n",
    "                            max_index = np.unravel_index(\n",
    "                                input_window.argmax(), input_window.shape)\n",
    "                            max_index = (i, j, h*self.stride +\n",
    "                                         max_index[0], w*self.stride+max_index[1])\n",
    "\n",
    "                            # add overlapped gradients\n",
    "                            # https://ai.stackexchange.com/a/17109\n",
    "                            input_error[max_index] += output_error[i, j, h, w]\n",
    "\n",
    "        # # clear cache\n",
    "        # input = None\n",
    "        # max_value_mask = None\n",
    "        # self.cache = None\n",
    "        return input_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# anan maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSlidingWindow(input, output_size, filter_dim, padding=0, stride=1, dilate=0):\n",
    "    X = input\n",
    "\n",
    "    if dilate != 0:\n",
    "        for _ in range(dilate):\n",
    "            X = np.insert(X, range(1, X.shape[2]), 0, axis=2)\n",
    "            X = np.insert(X, range(1, X.shape[3]), 0, axis=3)\n",
    "\n",
    "    if padding != 0:\n",
    "        X = np.pad(X, pad_width=((0,), (0,), (padding,), (padding,)), mode='constant', constant_values=(0.,))\n",
    "\n",
    "    in_b, in_c, out_h, out_w = output_size\n",
    "    out_b, out_c, _, _ = input.shape\n",
    "    batch_str, channel_str, filter_height_str, filter_width_str = X.strides\n",
    "\n",
    "    window = np.lib.stride_tricks.as_strided(X, shape=(out_b, out_c, out_h, out_w, filter_dim, filter_dim), strides=(batch_str, channel_str, filter_height_str * stride, filter_width_str * stride, filter_height_str, filter_width_str))\n",
    "    return window\n",
    "\n",
    "class AN_MaxPooling:\n",
    "    def __init__(self, filter_dim, stride=1):\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.input_data = None\n",
    "        self.window = None\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        batch_size, in_channels, height, width = input_data.shape\n",
    "\n",
    "        out_height = (height - self.filter_dim) // self.stride + 1\n",
    "        out_width = (width - self.filter_dim) // self.stride + 1\n",
    "\n",
    "        window = getSlidingWindow(input_data, (batch_size, in_channels, out_height, out_width), self.filter_dim, stride=self.stride)\n",
    "        self.window = window\n",
    "\n",
    "        output = np.max(window, axis=(4, 5))\n",
    "\n",
    "        maxs = output.repeat(self.stride, axis=2).repeat(self.stride, axis=3)\n",
    "        x_window = self.input_data[:, :, :out_height * self.stride, :out_width * self.stride]\n",
    "\n",
    "        mask = (x_window == maxs)\n",
    "        self.mask = mask\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_grad, learning_rate=0.01):\n",
    "        X = self.input_data\n",
    "        batch_size, in_channels, height, width = X.shape\n",
    "        h_pool, w_pool = self.filter_dim, self.filter_dim\n",
    "\n",
    "        mask = self.mask\n",
    "        dA = output_grad.repeat(h_pool, axis=2).repeat(w_pool, axis=3)\n",
    "        dA = np.multiply(dA, mask)\n",
    "\n",
    "        pad = np.zeros(X.shape)\n",
    "        pad[:, :, :dA.shape[2], :dA.shape[3]] = dA\n",
    "\n",
    "        return pad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 24, 24)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "asif_mp = AS_MaxPooling(filter_dim=(2,2), stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "asif_z = asif_mp.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 12, 12)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asif_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "utsa_mp = UT_Maxpool(filter_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "utsa_z = utsa_mp.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utsa_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(asif_z, utsa_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "anan_mp = MaxPooling(filter_dim=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "anan_z = anan_mp.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(asif_z, anan_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "asif_dx = asif_mp.backward(asif_z, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "utsa_dx = utsa_mp.backward(utsa_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "anan_dx = anan_mp.backward(anan_z, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(asif_dx, utsa_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(asif_dx, anan_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df95319d8ce4e1d89f5365ae10992bc1f65da593082b1d264e8f529830ec2f02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
